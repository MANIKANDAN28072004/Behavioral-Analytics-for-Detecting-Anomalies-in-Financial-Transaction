# -*- coding: utf-8 -*-
"""Blossom Bank Fraud Detection Machine Learning Model....ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nxY2NGMJvTcsj6B06mGxfu3L7mGS9-RT

# Behavioral Analytics for Detecting Anomalies in Financial Transaction
 Chosen Dataset: https://drive.google.com/file/d/1ZIjmAjPccvy16mOk7nrPtWe-_3rRP5zy/view?usp=sharing

With this model:
1. Keep up with fast evolving technological threats and better prevent the loss of funds (profit) to fraudsters.
2. Accurately detect and identify anomalies in
\\managing online transactions done on its platforms which may go undetected using traditional rules-based methods.
3. Improve quality assurance thus retaining old customers and acquire new ones. This will increase credit/profit base.
4. Improve its policy and decision making.
"""

# Commented out IPython magic to ensure Python compatibility.
#import the libaries to work with for EDA
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

#loading the data set
data = pd.read_csv('./Online Payment Fraud Detection.csv',encoding='unicode-escape')



"""# Understanding the Data"""

# Checking the size of the dataset (Rows,Columns)
data.shape

data.head()

data.tail()

"""Dataset contains transactions that happened within 3days:20hours"""

data.describe()

data.columns

data.info()

data.dtypes

#check for missing Data using isna().sum()

data.isna().sum()

"""No Missing Data

## Relationships and Insights

![https://th.bing.com/th/id/OIP.HgLmxNEHZnbeVAXWNzgv8AHaHa?pid=ImgDet&rs=1](attachment:image.png)

Exploring Relationships and insights in the dataset to answer unique business questions and gain more insights
"""

#Checking which recipients stand out

data.nameDest.unique()

#Investigating to check unique customers
data.nameOrig.unique()

#investigating to see how many times a customer started a transaction
data.nameOrig.value_counts()

#How many times a recipient got a transaction
data.nameDest.value_counts()

data.amount.max()

#Checking the distribution of the type of transactions made

labels = data['type'].astype('category').cat.categories.tolist()
counts = data['type'].value_counts()
sizes = [counts[var_cat] for var_cat in labels]
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot
ax1.axis('equal')
plt.show()

#Investigating how many times a particular type of transaction was carried out.

data.type.value_counts()

#Investigating the top customers and the type of transactions they initiated
top_ten = data.groupby('nameOrig').type.sum().sort_values(ascending=False)[:10]
top_ten

#Checking the average amounttransacted
data['amount'].mean()

sns.boxplot(y=data.step)
plt.title('Time of Transaction Profile')
plt.ylim(0,100)
plt.show()

sns.boxplot(y=data.amount)
plt.title('Amounts Transacted Profile')
plt.ylim(0,1000000)
plt.show()

sns.boxplot(y=data.isFraud)
plt.title('Fraud Profile')
plt.ylim(-1,1)
plt.show()

#Visualising the spread of fraud variables across the dataset

Online_Payment_layout = sns.PairGrid(data, vars = ['step', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest'], hue = 'isFraud')

Online_Payment_layout.map_diag(plt.hist, alpha = 0.6)
Online_Payment_layout.map_offdiag(plt.scatter, alpha = 0.5)
Online_Payment_layout.add_legend()

sns.barplot(x='amount', y='type', hue= 'isFraud', data=data)
plt.show()

sns.catplot(data=data,kind='box')

plt.ylim(0,2000000)

# Fraud Distribution

labels = data['isFraud'].astype('category').cat.categories.tolist()
counts = data['isFraud'].value_counts()
sizes = [counts[var_cat] for var_cat in labels]
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot
ax1.axis('equal')
plt.show()

# Sepearating Fraudulent transaction from non fraudulent

Fraudulent_Transaction = data[data.isFraud ==1]
Not_Fraudulent_Transaction = data[data.isFraud ==0]

print('Fraudulent Transaction: {}'.format(len(Fraudulent_Transaction)))
print('Not Fraudulent Transaction: {}'.format(len(Not_Fraudulent_Transaction)))

"""The dataset shows heavy 'Class-imbalance'"""

#Understanding The statistical nature of Non Fraudulent Transactions.

Not_Fraudulent_Transaction.amount.describe()

#Understanding The statistical nature of Fraudulent Transactions.

Fraudulent_Transaction.amount.describe()

#Comparing both class of transactions
data.groupby('isFraud').mean()

"""# Undersampling

Building a sample dataset containing similar distribution of Non Fraudulent transactions and Fraudulent transactions. This rectifies the class-imbalance and helps our machine learning model to perform better predictions.

Since we want to build a model that detects Fraud, we base our sampling on the number of transactions classed as fraudulent in our original dataset.

Fraudulent transactions: 1142
"""

Non_Fraudulent_Sample = Not_Fraudulent_Transaction.sample(n=1142)

"""Joining the two dataframes"""

new_dataset = pd.concat([Non_Fraudulent_Sample, Fraudulent_Transaction], axis=0)

new_dataset.head()

new_dataset.tail()

new_dataset['isFraud'].value_counts()

new_dataset.shape

#Checking to see if the new dataset we obtained is a good i.e does not deviate significantly from our original dataset
new_dataset.groupby('isFraud').mean()

"""New dataset obtained is good and class-imbalance rectified.

Now, the data can be split for feature selection, targeting and training.

# Feature Engineering

To train and test our machine learning model, we select features (columns) from our dataset  
Because our dataset contains categorial values ('nameOrig', 'nameDest'). it becomes necessary to convert them into binary format which can be readily used by or Machine learning model.

This is done through one-hot encoding.
"""

# One-hot Encoding.
#Importing library

from sklearn.preprocessing import OneHotEncoder

#creating instance of one-hot-encoder
encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, drop=None,)

#perform one-hot encoding on 'type' column
encoder_df =  pd.get_dummies(new_dataset, columns=['type','nameOrig','nameDest'], prefix=['type','nameOrig','nameDest'])

encoder_df

encoder_df.shape

# Check result of one-hot encoding
encoder_df.head()

encoder_df.tail()

"""# Target, Feature Split

Target Selection


'isFraud' column selected as Target (Y)
"""

Y = encoder_df['isFraud']

features = encoder_df.drop('isFraud', axis=1)

X = features

Y.head()

X.head()

from sklearn.model_selection import train_test_split

#create X_train, X_test, Y_train, Y_test
# using test_size of 20%

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, stratify=Y, random_state=2)

print('\n',X_train.head(2))

print('\n',X_test.head(2))

print('\n',Y_train.head(2))

print('\n',Y_test.head(2))

"""# Training our Classification Model with a Logistic Regression Classifier"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

#Training model with Training data
model.fit(X_train, Y_train)

model_pred = model.predict(X_test)

# Obtain model probabilities
probs = model.predict_proba(X_test)

"""## Logistic Regression Model Evaluation"""

#importing the methods
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score, precision_recall_curve, average_precision_score, roc_auc_score

print('\nClassification Report:')
print(classification_report(Y_test, model_pred))

# check True Negatives/Positives, False Negatives/Positives
pd.DataFrame(confusion_matrix(Y_test, model_pred),
             columns=['Predicted Negative(0) ', 'Predicted Positive(1)'],
             index=['Actually Negative(0)', 'Actually Positive(1)'])

# Print confusion matrix using predictions in context
pd.DataFrame(confusion_matrix(Y_test, model_pred),
             columns=['Predicted Not Fraud(0) ', 'Predicted Fraud(1)'],
             index=['Actually Not Fraud(0)', 'Actually Fraud(1)'])

# ACCURACY SCORE
print('Accuracy:',accuracy_score(Y_test, model_pred))

"""OurLogistic Regression Model indicates 93% accuracy"""

# Calculate average precision and the P-R curve
average_precision = average_precision_score(Y_test, model_pred)
average_precision

#define metrics
y_pred_proba = model.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)
auc = metrics.roc_auc_score(Y_test, y_pred_proba)


#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

print('AUC Score:')
print(roc_auc_score(Y_test, probs[:,1]))

"""The  ROC curve, also known as Receiver Operating Characteristics Curve, is a metric used to measure the performance of a classifier model. The ROC curve depicts the rate of true positives with respect to the rate of false positives, therefore highlighting the sensitivity of the classifier model. The higher the AUC (Area Under The Curve), the better the performance of the model at distinguishing between the positive and negative classes. It is a better metric than accuracy score.

An AUC score of 0.97 suggests that 97% of our prdictions will be correct.

## Training with Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier

# Define the model as the random forest
model = RandomForestClassifier(random_state=5, n_estimators=20)

model.fit(X_train,Y_train)

model_pred = model.predict(X_test)

# Obtain model probabilities
probs = model.predict_proba(X_test)

"""##  Random Forest Model Evaluation"""

#importing the methods
from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score, precision_recall_curve, average_precision_score, roc_auc_score

# Print classification report using predictions
print('Classification_Report:\n',classification_report(Y_test, model_pred))

# Print confusion matrix using predictions
pd.DataFrame(confusion_matrix(Y_test, model_pred),
             columns=['Predicted Negative(0) ', 'Predicted Positive(1)'],
             index=['Actually Negative(0)', 'Actually Positive(1)'])

# Print confusion matrix using predictions in Context
pd.DataFrame(confusion_matrix(Y_test, model_pred),
             columns=['Predicted Not Fraud(0) ', 'Predicted Fraud(1)'],
             index=['Actually Not Fraud(0)', 'Actually Fraud(1)'])

# ACCURACY SCORE
print('Accuracy:',accuracy_score(Y_test, model_pred))

"""Our Random Forest Model indicates 93% accuracy"""

# Calculate average precision and the P-R curve
average_precision = average_precision_score(Y_test, model_pred)
average_precision

#define metrics
y_pred_proba = model.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)
auc = metrics.roc_auc_score(Y_test, y_pred_proba)


#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()



# Print ROC_AUC score using probabilities
print('AUC Score:')
print(roc_auc_score(Y_test, probs[:, 1]))

"""An AUC score of 0.99 suggests that our Random Forest Model is almost perfect with 99% correct Predictions."""



